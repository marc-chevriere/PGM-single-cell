{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from anndata import AnnData\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.distributions import Normal, NegativeBinomial, Distribution\n",
    "from torch.distributions import kl_divergence as kl\n",
    "\n",
    "import scvi\n",
    "from scvi.model import SCVI\n",
    "from scvi.data import AnnDataManager\n",
    "from scvi import REGISTRY_KEYS\n",
    "from scvi.module.base import (\n",
    "    BaseModuleClass,\n",
    "    LossOutput,\n",
    "    auto_move_data,\n",
    ")\n",
    "from scvi.model.base import BaseModelClass, UnsupervisedTrainingMixin, VAEMixin\n",
    "from scvi.module import VAE\n",
    "from scvi.data.fields import (\n",
    "    CategoricalJointObsField,\n",
    "    CategoricalObsField,\n",
    "    LayerField,\n",
    "    NumericalJointObsField,\n",
    "    NumericalObsField,\n",
    ")\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from collections.abc import Iterator, Sequence\n",
    "import numpy.typing as npt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    project='PGM-single-cell', \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 100)\n"
     ]
    }
   ],
   "source": [
    "# adata = scvi.data.mouse_ob_dataset()\n",
    "# adata = scvi.data.purified_pbmc_dataset()\n",
    "\n",
    "# adata = adata[np.random.choice(adata.shape[0], size=adata.shape[0] // 50, replace=False)].copy()\n",
    "# n_genes_to_keep = adata.shape[1] // 4  # Conserver 1/4 des gènes\n",
    "# genes_indices = np.random.choice(adata.shape[1], size=n_genes_to_keep, replace=False)\n",
    "# adata = adata[:, genes_indices].copy()\n",
    "\n",
    "adata = scvi.data.synthetic_iid()\n",
    "\n",
    "print(adata.X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, n_latent: int, n_output: int, n_hidden: int = 128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_latent, n_hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.bn2 = nn.BatchNorm1d(n_hidden)\n",
    "        self.output_mean = nn.Linear(n_hidden, n_output)\n",
    "        self.output_disp = nn.Linear(n_hidden, n_output)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, z: torch.Tensor):\n",
    "        h = self.dropout(torch.relu(self.bn1(self.fc1(z))))\n",
    "        h = self.dropout(torch.relu(self.bn2(self.fc2(h))))\n",
    "        mean = torch.nn.functional.softplus(self.output_mean(h))\n",
    "        disp = torch.nn.functional.softplus(self.output_disp(h))\n",
    "        return mean, disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self, n_input: int, n_latent: int, n_hidden: int = 128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_input, n_hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.bn2 = nn.BatchNorm1d(n_hidden)\n",
    "        self.mean_layer = nn.Linear(n_hidden, n_latent)\n",
    "        self.var_layer = nn.Linear(n_hidden, n_latent)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        h = torch.relu(self.fc2(h))\n",
    "        mean = self.mean_layer(h)\n",
    "        log_var = self.var_layer(h)\n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVAEModule(BaseModuleClass):\n",
    "    \"\"\"Simple Variational auto-encoder model.\n",
    "\n",
    "    Here we implement a basic version of scVI's underlying VAE [Lopez18]_.\n",
    "    This implementation is for instructional purposes only.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        Number of input genes.\n",
    "    n_latent\n",
    "        Dimensionality of the latent space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_latent: int = 10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = SimpleEncoder(n_input, n_latent)\n",
    "        self.decoder = SimpleDecoder(n_latent, n_input)\n",
    "\n",
    "\n",
    "    def _get_inference_input(self, tensors: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"Parse the dictionary to get appropriate args\"\"\"\n",
    "        # let us fetch the raw counts, and add them to the dictionary\n",
    "        return {\"x\": tensors[REGISTRY_KEYS.X_KEY]}\n",
    "\n",
    "    @auto_move_data\n",
    "    def inference(self, x: torch.Tensor) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        High level inference method.\n",
    "\n",
    "        Runs the inference (encoder) model.\n",
    "        \"\"\"\n",
    "        x_ = torch.log1p(x)\n",
    "        qz_m, qz_v_log = self.encoder(x_)\n",
    "        qz_v = qz_v_log.exp()\n",
    "        z = Normal(qz_m, torch.sqrt(qz_v)).rsample()\n",
    "\n",
    "        return {\"qzm\": qz_m, \"qzv\": qz_v, \"z\": z}\n",
    "\n",
    "    def _get_generative_input(\n",
    "        self, tensors: dict[str, torch.Tensor], inference_outputs: dict[str, torch.Tensor]\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        return {\n",
    "            \"z\": inference_outputs[\"z\"],\n",
    "            # \"library\": torch.sum(tensors[REGISTRY_KEYS.X_KEY], dim=1, keepdim=True),\n",
    "        }\n",
    "\n",
    "    @auto_move_data\n",
    "    def generative(self, z: torch.Tensor) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"Runs the generative model.\"\"\"\n",
    "        nb_mean, nb_disp = self.decoder(z)\n",
    "        return {\n",
    "            \"nb_mean\":nb_mean,\n",
    "            \"nb_disp\":nb_disp,\n",
    "        }\n",
    "\n",
    "    def loss(\n",
    "        self,\n",
    "        tensors: dict[str, torch.Tensor],\n",
    "        inference_outputs: dict[str, torch.Tensor],\n",
    "        generative_outputs: dict[str, torch.Tensor],\n",
    "    ) -> LossOutput:\n",
    "        x = tensors[REGISTRY_KEYS.X_KEY]\n",
    "        nb_mean = generative_outputs[\"nb_mean\"]\n",
    "        nb_disp = generative_outputs[\"nb_disp\"]\n",
    "        qz_m = inference_outputs[\"qzm\"]\n",
    "        qz_v = inference_outputs[\"qzv\"]\n",
    "\n",
    "        log_likelihood = NegativeBinomial(total_count=nb_disp, logits=torch.log(nb_mean+1e-4)).log_prob(x).sum(dim=-1)\n",
    "\n",
    "        prior_dist = Normal(torch.zeros_like(qz_m), torch.ones_like(qz_v))\n",
    "        var_post_dist = Normal(qz_m, torch.sqrt(qz_v))\n",
    "        kl_divergence = kl(var_post_dist, prior_dist).sum(dim=-1)\n",
    "\n",
    "        elbo = log_likelihood - kl_divergence\n",
    "        loss = torch.mean(-elbo)\n",
    "        return LossOutput(\n",
    "            loss=loss,\n",
    "            reconstruction_loss=-log_likelihood,\n",
    "            kl_local=kl_divergence,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVAEModel(VAEMixin, UnsupervisedTrainingMixin, BaseModelClass):\n",
    "    \"\"\"single-cell Variational Inference [Lopez18]_.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata: AnnData,\n",
    "        n_latent: int = 10,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        super().__init__(adata)\n",
    "\n",
    "        self.module = SimpleVAEModule(\n",
    "            n_input=self.summary_stats[\"n_vars\"],\n",
    "            # n_batch=self.summary_stats[\"n_batch\"],\n",
    "            n_latent=n_latent,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        self._model_summary_string = (\n",
    "            f\"SCVI Model with the following params: \\nn_latent: {n_latent}\"\n",
    "        )\n",
    "        self.init_params_ = self._get_init_params(locals())\n",
    "\n",
    "    @classmethod\n",
    "    def setup_anndata(\n",
    "        cls,\n",
    "        adata: AnnData,\n",
    "        batch_key: str | None = None,\n",
    "        layer: str | None = None,\n",
    "        **kwargs,\n",
    "    ) -> AnnData | None:\n",
    "        setup_method_args = cls._get_setup_method_args(**locals())\n",
    "        anndata_fields = [\n",
    "            LayerField(REGISTRY_KEYS.X_KEY, layer, is_count_data=True),\n",
    "            CategoricalObsField(REGISTRY_KEYS.BATCH_KEY, batch_key),\n",
    "            # Dummy fields required for VAE class.\n",
    "            CategoricalObsField(REGISTRY_KEYS.LABELS_KEY, None),\n",
    "            NumericalObsField(REGISTRY_KEYS.SIZE_FACTOR_KEY, None, required=False),\n",
    "            CategoricalJointObsField(REGISTRY_KEYS.CAT_COVS_KEY, None),\n",
    "            NumericalJointObsField(REGISTRY_KEYS.CONT_COVS_KEY, None),\n",
    "        ]\n",
    "        adata_manager = AnnDataManager(fields=anndata_fields, setup_method_args=setup_method_args)\n",
    "        adata_manager.register_fields(adata, **kwargs)\n",
    "        cls.register_manager(adata_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SCVI Model with the following params: \n",
       "n_latent: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>\n",
       "Training status: Not Trained\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SCVI Model with the following params: \n",
       "n_latent: \u001b[1;36m10\u001b[0m\n",
       "Training status: Not Trained\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SimpleVAEModel.setup_anndata(adata, batch_key=\"batch\")\n",
    "simple_vae = SimpleVAEModel(adata, n_latent=10)\n",
    "simple_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/marc/mambaforge/envs/PGM/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/marc/mambaforge/envs/PGM/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/marc/mambaforge/envs/PGM/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7b4d46f0414892b26ce7655a902179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "# logger = wandb_logger\n",
    "logger = None\n",
    "\n",
    "simple_vae.train(max_epochs=50, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"saved_model_dir/saved_model_iid\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model_dir = os.path.join(save_dir, \"simple_vae_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_vae.save(model_dir, save_anndata=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m File saved_model_dir/saved_model_iid/simple_vae_model/model.pt already downloaded                         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marc/mambaforge/envs/PGM/lib/python3.12/site-packages/scvi/model/base/_save_load.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "simple_vae = SimpleVAEModel.load(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GM-VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderXtoY(nn.Module):\n",
    "    def __init__(self, n_input: int, n_clusters: int, n_hidden: int = 128):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),     \n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(n_hidden, n_clusters),\n",
    "            nn.Softmax(dim=-1),       \n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        probs_y = self.mlp(x)\n",
    "        return probs_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderXYtoZ(nn.Module):\n",
    "    def __init__(self, n_input: int, n_clusters: int, n_latent: int, n_hidden: int = 128):\n",
    "        super().__init__()\n",
    "        self.proj_y = nn.Sequential(\n",
    "            nn.Linear(n_clusters, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden)\n",
    "        )\n",
    "        self.proj_x = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden)\n",
    "        )\n",
    "        self.commonlayer = nn.Sequential(\n",
    "            nn.Linear(n_hidden * 2, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "        )\n",
    "        self.output_mean = nn.Linear(n_hidden, n_latent)\n",
    "        self.output_logvar = nn.Linear(n_hidden, n_latent)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        proj_x = self.proj_x(x)\n",
    "        proj_y = self.proj_y(y)\n",
    "        xy = torch.cat((proj_x,proj_y), dim=-1)\n",
    "        h = self.commonlayer(xy)\n",
    "        mean_n = self.output_mean(h)\n",
    "        logvar_n = self.output_logvar(h)\n",
    "        return mean_n, logvar_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderYtoZ(nn.Module):\n",
    "    def __init__(self, n_clusters: int, n_latent: int, n_hidden: int = 128):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_clusters, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),   \n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1)\n",
    "        )\n",
    "        self.output_mean_n = nn.Linear(n_hidden, n_latent)\n",
    "        self.output_logvar_n = nn.Linear(n_hidden, n_latent) \n",
    "\n",
    "    def forward(self, probs_y: torch.Tensor):\n",
    "        h = self.mlp(probs_y)\n",
    "        mean_n = self.output_mean_n(h)\n",
    "        logvar_n = self.output_logvar_n(h)\n",
    "        return mean_n, logvar_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderZtoX(nn.Module):\n",
    "    def __init__(self, n_output: int, n_latent: int, n_hidden: int = 128):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_latent, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden), \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),        \n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "        )     \n",
    "        self.output_mean = nn.Linear(n_hidden, n_output)\n",
    "        self.output_disp = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, z: torch.Tensor):\n",
    "        h = self.mlp(z)\n",
    "        mean = torch.nn.functional.softplus(self.output_mean(h))  \n",
    "        disp = torch.nn.functional.softplus(self.output_disp(h)) \n",
    "        return mean, disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMVAEModule(BaseModuleClass):\n",
    "    \"\"\"GM Variational auto-encoder model.\n",
    "\n",
    "    Here we implement a basic version of scVI's underlying VAE [Lopez18]_.\n",
    "    This implementation is for instructional purposes only.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        Number of input genes.\n",
    "    n_latent\n",
    "        Dimensionality of the latent space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_clusters: int,\n",
    "        n_latent: int = 10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoderxtoy = EncoderXtoY(n_input=n_input, n_clusters=n_clusters)\n",
    "        self.encoderxytoz = EncoderXYtoZ(n_clusters=n_clusters, n_input=n_input, n_latent=n_latent)\n",
    "        self.mu_y = nn.Parameter(torch.randn(n_clusters, n_latent))\n",
    "        self.logvar_y = nn.Parameter(torch.zeros(n_clusters, n_latent)) \n",
    "        self.decoderztox = DecoderZtoX(n_output=n_input, n_latent=n_latent)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_latent = n_latent\n",
    "\n",
    "\n",
    "    def _get_inference_input(self, tensors: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n",
    "        return {\"x\": tensors[REGISTRY_KEYS.X_KEY]}\n",
    "\n",
    "    @auto_move_data\n",
    "    def inference(self, x: torch.Tensor) -> dict[str, torch.Tensor]:\n",
    "        x_ = torch.log1p(x) \n",
    "        probs_y = self.encoderxtoy(x_)\n",
    "        y_one_hot = torch.eye(self.n_clusters, device=x.device).unsqueeze(0).repeat(x_.size(0), 1, 1)  # (batch_size, n_clusters, n_clusters)\n",
    "        x_expanded = x_.unsqueeze(1).repeat(1, self.n_clusters, 1)  # (batch_size, n_clusters, n_input)\n",
    "        mean_n, logvar_n = self.encoderxytoz(\n",
    "            x=x_expanded.view(-1, x_.size(-1)),  # Fusion des dimensions pour le traitement batch\n",
    "            y=y_one_hot.view(-1, self.n_clusters),  # Idem pour y\n",
    "        )\n",
    "        mean_n = mean_n.view(x_.size(0), self.n_clusters, -1)  # (batch_size, n_clusters, n_latent)\n",
    "        logvar_n = logvar_n.view(x_.size(0), self.n_clusters, -1)  # (batch_size, n_clusters, n_latent)\n",
    "        var_n = logvar_n.exp()\n",
    "        z_normales = Normal(mean_n, torch.sqrt(var_n)).rsample()  # (batch_size, n_clusters, n_latent)\n",
    "\n",
    "        return {\n",
    "            \"qzm\": mean_n,\n",
    "            \"qzv\": var_n,\n",
    "            \"z\": z_normales,\n",
    "            \"probs_y\": probs_y,\n",
    "            \"z_normales\": z_normales\n",
    "        }\n",
    "\n",
    "    def _get_generative_input(\n",
    "        self, tensors: dict[str, torch.Tensor], inference_outputs: dict[str, torch.Tensor]\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        return {\n",
    "            \"z_normales\": inference_outputs[\"z_normales\"],\n",
    "        }\n",
    "\n",
    "    @auto_move_data\n",
    "    def generative(self, z_normales: torch.Tensor) -> dict[str, torch.Tensor]:\n",
    "        z_flat = z_normales.view(-1, z_normales.size(-1))  # (batch_size * n_clusters, n_latent)\n",
    "        nb_mean, nb_disp = self.decoderztox(z_flat)  # (batch_size * n_clusters, n_output)\n",
    "        nb_mean = nb_mean.view(z_normales.size(0), z_normales.size(1), -1)\n",
    "        nb_disp = nb_disp.view(z_normales.size(0), z_normales.size(1), -1)\n",
    "\n",
    "        return {\n",
    "            \"nb_mean\": nb_mean,  # (batch_size, n_clusters, n_output)\n",
    "            \"nb_disp\": nb_disp,  # (batch_size, n_clusters, n_output)\n",
    "        }\n",
    "\n",
    "    def loss(\n",
    "        self,\n",
    "        tensors: dict[str, torch.Tensor],\n",
    "        inference_outputs: dict[str, torch.Tensor],\n",
    "        generative_outputs: dict[str, torch.Tensor],\n",
    "    ) -> LossOutput:\n",
    "        x = tensors[REGISTRY_KEYS.X_KEY]\n",
    "        batch_size = x.shape[0]\n",
    "        nb_mean = generative_outputs[\"nb_mean\"] # (batch_size, n_clusters, n_output)\n",
    "        nb_disp = generative_outputs[\"nb_disp\"] # (batch_size, n_clusters, n_output)\n",
    "\n",
    "        qz_m = inference_outputs[\"qzm\"] # (batch_size, n_clusters, n_latent)\n",
    "        qz_v = inference_outputs[\"qzv\"] # (batch_size, n_clusters, n_latent)\n",
    "        z_normales = inference_outputs[\"z\"]  # (batch_size, n_clusters, n_latent)\n",
    "        probs_y = inference_outputs[\"probs_y\"] # (batch_size, n_clusters)\n",
    "\n",
    "        x_expanded = x.unsqueeze(1)\n",
    "        log_likelihood = NegativeBinomial(total_count=nb_disp, logits=torch.log(nb_mean+1e-4)).log_prob(x_expanded).sum(dim=-1) # (batch_size, n_clusters)\n",
    "        mu_y_expanded = self.mu_y.unsqueeze(0).expand(batch_size, -1, -1)  # (n_batch, n_clusters, n_latent)\n",
    "        var_y_expanded = self.logvar_y.unsqueeze(0).expand(batch_size, -1, -1).exp()  # (n_batch, n_clusters, n_latent)\n",
    "        priors_z_y_distributions = Normal(mu_y_expanded, torch.sqrt(var_y_expanded)) # (batch_size, n_clusters)\n",
    "        var_post_dist = Normal(qz_m, torch.sqrt(qz_v)) # (batch_size, n_clusters)\n",
    "        kl_div_1 = kl(var_post_dist, priors_z_y_distributions).sum(dim=-1) # (batch_size, n_clusters)\n",
    "\n",
    "        avg_cat_ll_kl = ((log_likelihood - kl_div_1) * probs_y).sum(dim=1) # (batch_size)\n",
    "\n",
    "        q_y_x = torch.distributions.Categorical(probs_y)\n",
    "        probs_uniform = torch.ones_like(probs_y)/self.n_clusters\n",
    "        unif_pi = torch.distributions.Categorical(probs_uniform)\n",
    "        kl_div_2 = kl(q_y_x, unif_pi).sum(dim=-1) # (batch_size)\n",
    "\n",
    "        elbo = avg_cat_ll_kl - kl_div_2 # (batch_size)\n",
    "        loss = torch.mean(-elbo)\n",
    "        return LossOutput(\n",
    "            loss=loss,\n",
    "            reconstruction_loss=-avg_cat_ll_kl,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMVAEModel(VAEMixin, UnsupervisedTrainingMixin, BaseModelClass):\n",
    "    \"\"\"single-cell Variational Inference [Lopez18]_.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata: AnnData,\n",
    "        n_clusters: int,\n",
    "        n_latent: int = 10,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        super().__init__(adata)\n",
    "\n",
    "        self.module = GMVAEModule(\n",
    "            n_input=self.summary_stats[\"n_vars\"],\n",
    "            # n_batch=self.summary_stats[\"n_batch\"],\n",
    "            n_clusters=n_clusters,\n",
    "            n_latent=n_latent,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        self._model_summary_string = (\n",
    "            f\"SCVI Model with the following params: \\nn_latent: {n_latent}\"\n",
    "        )\n",
    "        self.init_params_ = self._get_init_params(locals())\n",
    "\n",
    "\n",
    "    def get_latent_representation(\n",
    "        self,\n",
    "        adata: AnnData | None = None,\n",
    "        indices: Sequence[int] | None = None,\n",
    "        batch_size: int | None = None,\n",
    "        dataloader: Iterator[dict[str, Tensor | None]] = None,\n",
    "    ):\n",
    "\n",
    "        self._check_if_trained(warn=False)\n",
    "        if adata is not None and dataloader is not None:\n",
    "            raise ValueError(\"Only one of `adata` or `dataloader` can be provided.\")\n",
    "\n",
    "        if dataloader is None:\n",
    "            adata = self._validate_anndata(adata)\n",
    "            dataloader = self._make_data_loader(\n",
    "                adata=adata, indices=indices, batch_size=batch_size\n",
    "            )\n",
    "        latent = {}\n",
    "        latent_rep = []\n",
    "        latent_cat = []\n",
    "        for tensors in dataloader:\n",
    "            inference_inputs = self.module._get_inference_input(tensors)\n",
    "            outputs = self.module.inference(**inference_inputs)\n",
    "            qz_m = outputs[\"qzm\"]\n",
    "            probs_y = outputs[\"probs_y\"]\n",
    "            latent_rep += [qz_m.cpu()]\n",
    "            latent_cat += [probs_y.cpu()]\n",
    "        latent[\"latent_rep\"] = torch.cat(latent_rep).detach().numpy()\n",
    "        latent[\"latent_cat\"] = torch.cat(latent_cat).detach().numpy()\n",
    "        return latent\n",
    "\n",
    "    @classmethod\n",
    "    def setup_anndata(\n",
    "        cls,\n",
    "        adata: AnnData,\n",
    "        batch_key: str | None = None,\n",
    "        layer: str | None = None,\n",
    "        **kwargs,\n",
    "    ) -> AnnData | None:\n",
    "        setup_method_args = cls._get_setup_method_args(**locals())\n",
    "        anndata_fields = [\n",
    "            LayerField(REGISTRY_KEYS.X_KEY, layer, is_count_data=True),\n",
    "            CategoricalObsField(REGISTRY_KEYS.BATCH_KEY, batch_key),\n",
    "            # Dummy fields required for VAE class.\n",
    "            CategoricalObsField(REGISTRY_KEYS.LABELS_KEY, None),\n",
    "            NumericalObsField(REGISTRY_KEYS.SIZE_FACTOR_KEY, None, required=False),\n",
    "            CategoricalJointObsField(REGISTRY_KEYS.CAT_COVS_KEY, None),\n",
    "            NumericalJointObsField(REGISTRY_KEYS.CONT_COVS_KEY, None),\n",
    "        ]\n",
    "        adata_manager = AnnDataManager(fields=anndata_fields, setup_method_args=setup_method_args)\n",
    "        adata_manager.register_fields(adata, **kwargs)\n",
    "        cls.register_manager(adata_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SCVI Model with the following params: \n",
       "n_latent: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>\n",
       "Training status: Not Trained\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SCVI Model with the following params: \n",
       "n_latent: \u001b[1;36m10\u001b[0m\n",
       "Training status: Not Trained\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GMVAEModel.setup_anndata(adata, batch_key=\"batch\")\n",
    "\n",
    "gm_vae = GMVAEModel(adata, n_clusters=10, n_latent=10)\n",
    "gm_vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/marc/mambaforge/envs/PGM/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/marc/mambaforge/envs/PGM/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/marc/mambaforge/envs/PGM/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ffc114501e458fb075610d593565f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "# logger = wandb_logger\n",
    "logger = None\n",
    "\n",
    "gm_vae.train(max_epochs=50, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12948938\n"
     ]
    }
   ],
   "source": [
    "print(np.max(gm_vae.get_latent_representation(adata)[\"latent_cat\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"saved_model_dir/saved_model_iid\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model_dir_gm = os.path.join(save_dir, \"gm_vae_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_vae.save(model_dir_gm, save_anndata=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m File saved_model_dir/saved_model_iid/gm_vae_model/model.pt already downloaded                             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marc/mambaforge/envs/PGM/lib/python3.12/site-packages/scvi/model/base/_save_load.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "gm_vae = GMVAEModel.load(model_dir_gm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE from SCVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/marc/mambaforge/envs/PGM/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/marc/mambaforge/envs/PGM/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/marc/mambaforge/envs/PGM/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5258e737614f408f38ba45876f2756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    }
   ],
   "source": [
    "scvi.model.SCVI.setup_anndata(adata, batch_key=\"batch\")\n",
    "\n",
    "# Initialiser le modèle scVI\n",
    "vae = scvi.model.SCVI(adata)\n",
    "\n",
    "# Entraîner le modèle\n",
    "vae.train(max_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"saved_model_dir\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model_dir_scvi = os.path.join(save_dir, \"scvi_vae_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.save(model_dir_scvi, save_anndata=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m File saved_model_dir/scvi_vae_model/model.pt already downloaded                                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marc/mambaforge/envs/PGM/lib/python3.12/site-packages/scvi/model/base/_save_load.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "vae = SCVI.load(model_dir_scvi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Scores de Clustering ===\n",
      "\n",
      "\u001b[34mINFO    \u001b[0m AnnData object appears to be a copy. Attempting to transfer setup.                                        \n",
      "VAE de scvi tools :\n",
      "  Adjusted Rand Index (ARI): -0.0027\n",
      "  Normalized Mutual Information (NMI): 0.0021\n",
      "\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData setup                             \n",
      "Simple VAE :\n",
      "  Adjusted Rand Index (ARI): -0.0021\n",
      "  Normalized Mutual Information (NMI): 0.0028\n",
      "\n",
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData setup                             \n",
      "GM VAE :\n",
      "  Adjusted Rand Index (ARI): 0.0013\n",
      "  Normalized Mutual Information (NMI): 0.0157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Affichage des scores ---\n",
    "print(\"=== Scores de Clustering ===\\n\")\n",
    "\n",
    "# Récupération des vrais labels\n",
    "true_labels = adata.obs['labels']\n",
    "\n",
    "# --- Clustering et évaluation pour le VAE scVI  ---\n",
    "latent_rep_vae = vae.get_latent_representation(adata)\n",
    "kmeans_vae = KMeans(n_clusters=len(set(true_labels)), n_init=200, random_state=42)\n",
    "predicted_labels_vae = kmeans_vae.fit_predict(latent_rep_vae)\n",
    "ari_vae = adjusted_rand_score(true_labels, predicted_labels_vae)\n",
    "nmi_vae = normalized_mutual_info_score(true_labels, predicted_labels_vae)\n",
    "print(\"VAE de scvi tools :\")\n",
    "print(f\"  Adjusted Rand Index (ARI): {ari_vae:.4f}\")\n",
    "print(f\"  Normalized Mutual Information (NMI): {nmi_vae:.4f}\\n\")\n",
    "\n",
    "# --- Clustering et évaluation pour le VAE personnalisé  ---\n",
    "latent_rep_svae = simple_vae.get_latent_representation(adata)\n",
    "kmeans_svae = KMeans(n_clusters=len(set(true_labels)), n_init=200, random_state=42)\n",
    "predicted_labels_svae = kmeans_svae.fit_predict(latent_rep_svae)\n",
    "ari_svae = adjusted_rand_score(true_labels, predicted_labels_svae)\n",
    "nmi_svae = normalized_mutual_info_score(true_labels, predicted_labels_svae)\n",
    "print(\"Simple VAE :\")\n",
    "print(f\"  Adjusted Rand Index (ARI): {ari_svae:.4f}\")\n",
    "print(f\"  Normalized Mutual Information (NMI): {nmi_svae:.4f}\\n\")\n",
    "\n",
    "\n",
    "# --- Clustering et évaluation pour le VAE personnalisé avec GM ---\n",
    "latent_cat_gmvae = gm_vae.get_latent_representation(adata)[\"latent_cat\"]\n",
    "predicted_labels_gmvae = np.argmax(latent_cat_gmvae, axis=-1)\n",
    "ari_gmvae = adjusted_rand_score(true_labels, predicted_labels_gmvae)\n",
    "nmi_gmvae = normalized_mutual_info_score(true_labels, predicted_labels_gmvae)\n",
    "print(\"GM VAE :\")\n",
    "print(f\"  Adjusted Rand Index (ARI): {ari_gmvae:.4f}\")\n",
    "print(f\"  Normalized Mutual Information (NMI): {nmi_gmvae:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PGM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
