{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from anndata import AnnData\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal, NegativeBinomial\n",
    "from torch.distributions import kl_divergence as kl\n",
    "\n",
    "import scvi\n",
    "from scvi.data import AnnDataManager\n",
    "from scvi import REGISTRY_KEYS\n",
    "from scvi.module.base import (\n",
    "    BaseModuleClass,\n",
    "    LossOutput,\n",
    "    auto_move_data,\n",
    ")\n",
    "from scvi.model.base import BaseModelClass, UnsupervisedTrainingMixin, VAEMixin\n",
    "from scvi.module import VAE\n",
    "from scvi.data.fields import (\n",
    "    CategoricalJointObsField,\n",
    "    CategoricalObsField,\n",
    "    LayerField,\n",
    "    NumericalJointObsField,\n",
    "    NumericalObsField,\n",
    ")\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    project='PGM-single-cell', \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m File data/PurifiedPBMCDataset.h5ad already downloaded                                                     \n",
      "(2117, 21932)\n"
     ]
    }
   ],
   "source": [
    "# adata = scvi.data.mouse_ob_dataset()\n",
    "adata = scvi.data.purified_pbmc_dataset()\n",
    "\n",
    "adata = adata[np.random.choice(adata.shape[0], size=adata.shape[0] // 50, replace=False)].copy()\n",
    "adata\n",
    "\n",
    "# adata = scvi.data.synthetic_iid()\n",
    "\n",
    "print(adata.X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, n_latent: int, n_output: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_latent, 128)  \n",
    "        self.fc2 = nn.Linear(128, 128)      \n",
    "        self.output_mean = nn.Linear(128, n_output)  \n",
    "        self.output_disp = nn.Linear(128, n_output)  \n",
    "\n",
    "    def forward(self, z: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Reconstruit les données depuis l'espace latent.\n",
    "        - z : Tensor de taille (batch_size, n_latent)\n",
    "        Retourne :\n",
    "        - mean : Moyenne de la distribution reconstruite\n",
    "        - disp : Dispersion (ou variance) de la distribution reconstruite\n",
    "        \"\"\"\n",
    "        h = torch.relu(self.fc1(z))\n",
    "        h = torch.relu(self.fc2(h))\n",
    "        mean = torch.nn.functional.softplus(self.output_mean(h))  \n",
    "        disp = torch.nn.functional.softplus(self.output_disp(h)) \n",
    "        return mean, disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self, n_input: int, n_latent: int, n_hidden: int = 128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_input, n_hidden)  \n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden) \n",
    "        self.mean_layer = nn.Linear(n_hidden, n_latent) \n",
    "        self.var_layer = nn.Linear(n_hidden, n_latent)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Encode les données d'entrée dans l'espace latent.\n",
    "        - x : Tensor des données (batch_size, n_input)\n",
    "        Retourne :\n",
    "        - mean : Moyenne latente\n",
    "        - log_var : Log-variance latente\n",
    "        \"\"\"\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        h = torch.relu(self.fc2(h))\n",
    "        mean = self.mean_layer(h)\n",
    "        log_var = self.var_layer(h)\n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVAEModule(BaseModuleClass):\n",
    "    \"\"\"Simple Variational auto-encoder model.\n",
    "\n",
    "    Here we implement a basic version of scVI's underlying VAE [Lopez18]_.\n",
    "    This implementation is for instructional purposes only.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        Number of input genes.\n",
    "    n_latent\n",
    "        Dimensionality of the latent space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_latent: int = 10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = SimpleEncoder(n_input, n_latent)\n",
    "        self.decoder = SimpleDecoder(n_latent, n_input)\n",
    "\n",
    "\n",
    "    def _get_inference_input(self, tensors: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"Parse the dictionary to get appropriate args\"\"\"\n",
    "        # let us fetch the raw counts, and add them to the dictionary\n",
    "        return {\"x\": tensors[REGISTRY_KEYS.X_KEY]}\n",
    "\n",
    "    @auto_move_data\n",
    "    def inference(self, x: torch.Tensor) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        High level inference method.\n",
    "\n",
    "        Runs the inference (encoder) model.\n",
    "        \"\"\"\n",
    "        x_ = torch.log1p(x)\n",
    "        qz_m, qz_v_log = self.encoder(x_)\n",
    "        qz_v = qz_v_log.exp()\n",
    "        z = Normal(qz_m, torch.sqrt(qz_v)).rsample()\n",
    "\n",
    "        return {\"qzm\": qz_m, \"qzv\": qz_v, \"z\": z}\n",
    "\n",
    "    def _get_generative_input(\n",
    "        self, tensors: dict[str, torch.Tensor], inference_outputs: dict[str, torch.Tensor]\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        return {\n",
    "            \"z\": inference_outputs[\"z\"],\n",
    "            # \"library\": torch.sum(tensors[REGISTRY_KEYS.X_KEY], dim=1, keepdim=True),\n",
    "        }\n",
    "\n",
    "    @auto_move_data\n",
    "    def generative(self, z: torch.Tensor) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"Runs the generative model.\"\"\"\n",
    "        nb_mean, nb_disp = self.decoder(z)\n",
    "        return {\n",
    "            \"nb_mean\":nb_mean,\n",
    "            \"nb_disp\":nb_disp,\n",
    "        }\n",
    "\n",
    "    def loss(\n",
    "        self,\n",
    "        tensors: dict[str, torch.Tensor],\n",
    "        inference_outputs: dict[str, torch.Tensor],\n",
    "        generative_outputs: dict[str, torch.Tensor],\n",
    "    ) -> LossOutput:\n",
    "        x = tensors[REGISTRY_KEYS.X_KEY]\n",
    "        nb_mean = generative_outputs[\"nb_mean\"]\n",
    "        nb_disp = generative_outputs[\"nb_disp\"]\n",
    "        qz_m = inference_outputs[\"qzm\"]\n",
    "        qz_v = inference_outputs[\"qzv\"]\n",
    "\n",
    "        log_likelihood = NegativeBinomial(total_count=nb_disp, logits=torch.log(nb_mean+1e-4)).log_prob(x).sum(dim=-1)\n",
    "\n",
    "        prior_dist = Normal(torch.zeros_like(qz_m), torch.ones_like(qz_v))\n",
    "        var_post_dist = Normal(qz_m, torch.sqrt(qz_v))\n",
    "        kl_divergence = kl(var_post_dist, prior_dist).sum(dim=1)\n",
    "        \n",
    "        elbo = log_likelihood - kl_divergence\n",
    "        loss = torch.mean(-elbo)\n",
    "        return LossOutput(\n",
    "            loss=loss,\n",
    "            reconstruction_loss=-log_likelihood,\n",
    "            kl_local=kl_divergence,\n",
    "            kl_global=0.0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEModel(VAEMixin, UnsupervisedTrainingMixin, BaseModelClass):\n",
    "    \"\"\"single-cell Variational Inference [Lopez18]_.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata: AnnData,\n",
    "        module: BaseModuleClass,\n",
    "        n_latent: int = 10,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        super().__init__(adata)\n",
    "\n",
    "        self.module = SimpleVAEModule(\n",
    "            n_input=self.summary_stats[\"n_vars\"],\n",
    "            # n_batch=self.summary_stats[\"n_batch\"],\n",
    "            n_latent=n_latent,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        self._model_summary_string = (\n",
    "            f\"SCVI Model with the following params: \\nn_latent: {n_latent}\"\n",
    "        )\n",
    "        self.init_params_ = self._get_init_params(locals())\n",
    "\n",
    "    @classmethod\n",
    "    def setup_anndata(\n",
    "        cls,\n",
    "        adata: AnnData,\n",
    "        batch_key: str | None = None,\n",
    "        layer: str | None = None,\n",
    "        **kwargs,\n",
    "    ) -> AnnData | None:\n",
    "        setup_method_args = cls._get_setup_method_args(**locals())\n",
    "        anndata_fields = [\n",
    "            LayerField(REGISTRY_KEYS.X_KEY, layer, is_count_data=True),\n",
    "            CategoricalObsField(REGISTRY_KEYS.BATCH_KEY, batch_key),\n",
    "            # Dummy fields required for VAE class.\n",
    "            CategoricalObsField(REGISTRY_KEYS.LABELS_KEY, None),\n",
    "            NumericalObsField(REGISTRY_KEYS.SIZE_FACTOR_KEY, None, required=False),\n",
    "            CategoricalJointObsField(REGISTRY_KEYS.CAT_COVS_KEY, None),\n",
    "            NumericalJointObsField(REGISTRY_KEYS.CONT_COVS_KEY, None),\n",
    "        ]\n",
    "        adata_manager = AnnDataManager(fields=anndata_fields, setup_method_args=setup_method_args)\n",
    "        adata_manager.register_fields(adata, **kwargs)\n",
    "        cls.register_manager(adata_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SCVI Model with the following params: \n",
       "n_latent: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>\n",
       "Training status: Not Trained\n",
       "</pre>\n"
      ],
      "text/plain": [
       "SCVI Model with the following params: \n",
       "n_latent: \u001b[1;36m10\u001b[0m\n",
       "Training status: Not Trained\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAEModel.setup_anndata(adata, batch_key=\"batch\")\n",
    "simple_vae = VAEModel(adata, module=SimpleVAEModule, n_latent=10)\n",
    "simple_vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/marc/mambaforge/envs/PGM/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/marc/mambaforge/envs/PGM/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d093636c7e410eaefb1e764171e0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n"
     ]
    }
   ],
   "source": [
    "# logger = wandb_logger\n",
    "logger = None\n",
    "\n",
    "simple_vae.train(max_epochs=40, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GM-VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMVAEModule(BaseModuleClass):\n",
    "    \"\"\"GM Variational auto-encoder model.\n",
    "\n",
    "    Here we implement a basic version of scVI's underlying VAE [Lopez18]_.\n",
    "    This implementation is for instructional purposes only.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        Number of input genes.\n",
    "    n_latent\n",
    "        Dimensionality of the latent space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_input: int,\n",
    "        n_latent: int = 10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = SimpleEncoder(n_input, n_latent)\n",
    "        self.decoder = SimpleDecoder(n_latent, n_input)\n",
    "\n",
    "\n",
    "    def _get_inference_input(self, tensors: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"Parse the dictionary to get appropriate args\"\"\"\n",
    "        # let us fetch the raw counts, and add them to the dictionary\n",
    "        return {\"x\": tensors[REGISTRY_KEYS.X_KEY]}\n",
    "\n",
    "    @auto_move_data\n",
    "    def inference(self, x: torch.Tensor) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        High level inference method.\n",
    "\n",
    "        Runs the inference (encoder) model.\n",
    "        \"\"\"\n",
    "        x_ = torch.log1p(x)\n",
    "        qz_m, qz_v_log = self.encoder(x_)\n",
    "        qz_v = qz_v_log.exp()\n",
    "        z = Normal(qz_m, torch.sqrt(qz_v)).rsample()\n",
    "\n",
    "        return {\"qzm\": qz_m, \"qzv\": qz_v, \"z\": z}\n",
    "\n",
    "    def _get_generative_input(\n",
    "        self, tensors: dict[str, torch.Tensor], inference_outputs: dict[str, torch.Tensor]\n",
    "    ) -> dict[str, torch.Tensor]:\n",
    "        return {\n",
    "            \"z\": inference_outputs[\"z\"],\n",
    "            # \"library\": torch.sum(tensors[REGISTRY_KEYS.X_KEY], dim=1, keepdim=True),\n",
    "        }\n",
    "\n",
    "    @auto_move_data\n",
    "    def generative(self, z: torch.Tensor) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"Runs the generative model.\"\"\"\n",
    "        nb_mean, nb_disp = self.decoder(z)\n",
    "        return {\n",
    "            \"nb_mean\":nb_mean,\n",
    "            \"nb_disp\":nb_disp,\n",
    "        }\n",
    "\n",
    "    def loss(\n",
    "        self,\n",
    "        tensors: dict[str, torch.Tensor],\n",
    "        inference_outputs: dict[str, torch.Tensor],\n",
    "        generative_outputs: dict[str, torch.Tensor],\n",
    "    ) -> LossOutput:\n",
    "        x = tensors[REGISTRY_KEYS.X_KEY]\n",
    "        nb_mean = generative_outputs[\"nb_mean\"]\n",
    "        nb_disp = generative_outputs[\"nb_disp\"]\n",
    "        qz_m = inference_outputs[\"qzm\"]\n",
    "        qz_v = inference_outputs[\"qzv\"]\n",
    "\n",
    "        log_likelihood = NegativeBinomial(total_count=nb_disp, logits=torch.log(nb_mean+1e-4)).log_prob(x).sum(dim=-1)\n",
    "\n",
    "        prior_dist = Normal(torch.zeros_like(qz_m), torch.ones_like(qz_v))\n",
    "        var_post_dist = Normal(qz_m, torch.sqrt(qz_v))\n",
    "        kl_divergence = kl(var_post_dist, prior_dist).sum(dim=1)\n",
    "        \n",
    "        elbo = log_likelihood - kl_divergence\n",
    "        loss = torch.mean(-elbo)\n",
    "        return LossOutput(\n",
    "            loss=loss,\n",
    "            reconstruction_loss=-log_likelihood,\n",
    "            kl_local=kl_divergence,\n",
    "            kl_global=0.0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAEModel.setup_anndata(adata, batch_key=\"batch\")\n",
    "\n",
    "# gm_vae = VAEModel(adata, module=GMVAEModule, n_latent=10)\n",
    "# gm_vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = wandb_logger\n",
    "# logger = None\n",
    "\n",
    "# gm_vae.train(max_epochs=40, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE from SCVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/marc/mambaforge/envs/PGM/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/marc/mambaforge/envs/PGM/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7d1f63c1504134b0f2054790a7c817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n"
     ]
    }
   ],
   "source": [
    "scvi.model.SCVI.setup_anndata(adata, batch_key=\"batch\")\n",
    "\n",
    "# Initialiser le modèle scVI\n",
    "vae = scvi.model.SCVI(adata)\n",
    "\n",
    "# Entraîner le modèle\n",
    "vae.train(max_epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Scores de Clustering ===\n",
      "\n",
      "VAE de scvi tools :\n",
      "  Adjusted Rand Index (ARI): 0.3263\n",
      "  Normalized Mutual Information (NMI): 0.5337\n",
      "\n",
      "Simple VAE :\n",
      "  Adjusted Rand Index (ARI): 0.0609\n",
      "  Normalized Mutual Information (NMI): 0.1624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Affichage des scores ---\n",
    "print(\"=== Scores de Clustering ===\\n\")\n",
    "\n",
    "# Récupération des vrais labels\n",
    "true_labels = adata.obs['labels']\n",
    "\n",
    "# --- Clustering et évaluation pour le VAE scVI  ---\n",
    "latent_rep_vae = vae.get_latent_representation(adata)\n",
    "kmeans_vae = KMeans(n_clusters=len(set(true_labels)), n_init=200, random_state=42)\n",
    "predicted_labels_vae = kmeans_vae.fit_predict(latent_rep_vae)\n",
    "ari_vae = adjusted_rand_score(true_labels, predicted_labels_vae)\n",
    "nmi_vae = normalized_mutual_info_score(true_labels, predicted_labels_vae)\n",
    "print(\"VAE de scvi tools :\")\n",
    "print(f\"  Adjusted Rand Index (ARI): {ari_vae:.4f}\")\n",
    "print(f\"  Normalized Mutual Information (NMI): {nmi_vae:.4f}\\n\")\n",
    "\n",
    "# --- Clustering et évaluation pour le VAE personnalisé  ---\n",
    "latent_rep_svae = simple_vae.get_latent_representation(adata)\n",
    "kmeans_svae = KMeans(n_clusters=len(set(true_labels)), n_init=200, random_state=42)\n",
    "predicted_labels_svae = kmeans_svae.fit_predict(latent_rep_svae)\n",
    "ari_svae = adjusted_rand_score(true_labels, predicted_labels_svae)\n",
    "nmi_svae = normalized_mutual_info_score(true_labels, predicted_labels_svae)\n",
    "print(\"Simple VAE :\")\n",
    "print(f\"  Adjusted Rand Index (ARI): {ari_svae:.4f}\")\n",
    "print(f\"  Normalized Mutual Information (NMI): {nmi_svae:.4f}\\n\")\n",
    "\n",
    "\n",
    "# # --- Clustering et évaluation pour le VAE personnalisé avec GM ---\n",
    "# latent_rep_gmvae = gm_vae.get_latent_representation(adata)\n",
    "# kmeans_gmvae = KMeans(n_clusters=len(set(true_labels)), n_init=200, random_state=42)\n",
    "# predicted_labels_gmvae = kmeans_gmvae.fit_predict(latent_rep_gmvae)\n",
    "# ari_gmvae = adjusted_rand_score(true_labels, predicted_labels_gmvae)\n",
    "# nmi_gmvae = normalized_mutual_info_score(true_labels, predicted_labels_gmvae)\n",
    "# print(\"GM VAE :\")\n",
    "# print(f\"  Adjusted Rand Index (ARI): {ari_gmvae:.4f}\")\n",
    "# print(f\"  Normalized Mutual Information (NMI): {nmi_gmvae:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PGM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
